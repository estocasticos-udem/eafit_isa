{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "S05_Bases_Estadisticas.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMX+q7sFmrL0nIrF1SRe99U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ssanchezgoe/eafit_isa/blob/main/Nb_Google_Colab/S05_Bases_Estadisticas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8XsvcYOlYfz"
      },
      "source": [
        "  <tr>\n",
        "     <th><p><img alt=\"Colaboratory logo\" height=\"80px\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/83/Sena_Colombia_logo.svg/1045px-Sena_Colombia_logo.svg.png\" align=\"left\" hspace=\"10px\" vspace=\"0px\"></p></th> \n",
        "    <th><p><img alt=\"Colaboratory logo\" height=\"80px\" src=\"https://www.isa.co/wp-content/uploads/2020/11/logo.png\" align=\"right\" hspace=\"10px\" vspace=\"0px\"></p></th>\n",
        "    <th><p><img alt=\"Colaboratory logo\" height=\"80px\" src=\"https://upload.wikimedia.org/wikipedia/commons/b/bf/EAFIT-2015.png\" align=\"left\" hspace=\"10px\" vspace=\"0px\"></p></th> \n",
        "     <th><h1>  Bases de Estadística Básica Aplicada </h1></th>\n",
        "  </tr>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBAGa-A40g6Q"
      },
      "source": [
        "<p><a name=\"contents\"></a></p>\n",
        "\n",
        "# Contenido Sesión: Introducción al Machine Learning.\n",
        "\n",
        "- <a href=\"#ML\">1. Modelos basados en datos y Machine Learning</a><br>\n",
        "  - <a href=\"#modelML\">1.1. Modelos que se pueden construir</code></a><br>\n",
        "  - <a href=\"#terms\">1.2. Terminología</code></a><br>\n",
        "  - <a href=\"#supML\">1.3. Aprendizaje supervisado</code></a><br>\n",
        "  - <a href=\"#nonsupML\">1.4. Aprendizaje no supervisado</code></a><br>\n",
        "  - <a href=\"#data\">1.5. Observaciones acerca de los datos</code></a><br>\n",
        "- <a href=\"#sklearn\">2. Introducción a Scikit-Learn</a><br>\n",
        "  - <a href=\"#sklearnG\">2.1 Generalidades</a><br>\n",
        "  - <a href=\"#sklearnT\">2.2. Tratamiento de datos</a><br>\n",
        "    - <a href=\"#sklearnTT\">2.2.1 Datos de entrenamiento y de prueba</a><br>\n",
        "    - <a href=\"#sklearnCT\">2.2.2 Conversión de variables categóricas</a><br>\n",
        "    - <a href=\"#sklearnFS\">2.2.3 Escalamiento de características</a><br>\n",
        "    - <a href=\"#sklearnPL\">2.2.4 Pipeline</a><br>\n",
        "- <a href=\"#taller\">3. Taller</a><br>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QE8JRK9e5uT6"
      },
      "source": [
        "<p><a name=\"ML\"></a></p>\n",
        "\n",
        "# 1. Modelos basados en datos y Machine Learning\n",
        "\n",
        "[Contenidos](#contents) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QON9ti9x6anA"
      },
      "source": [
        "<p><a name=\"modelML\"></a></p>\n",
        "\n",
        "## 1.1. Modelos que se pueden construir\n",
        "\n",
        "[Contenidos](#contents)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFOQl_5Qs_3e"
      },
      "source": [
        "¿Qué es un modelo derivado de los datos?\n",
        "\n",
        "Caso ideal: Sabemos las distribuciones de las que vienen los datos. ⇒ podemos calcular analíticamente nuestro modelo.\n",
        "\n",
        "¿Podemos encontrar un modelo con 100% de acierto? ¿Por qué sí, o por qué no?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pGwvGxGo2Ix"
      },
      "source": [
        "from scipy import stats\n",
        "from scipy import optimize\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "d1 = stats.norm(loc=10,scale=2)\n",
        "d2 = stats.norm(loc=17,scale=3)\n",
        "\n",
        "\n",
        "x = np.linspace(0,30,100)\n",
        "plt.plot(x, d1.pdf(x), color=\"red\", label=\"pop 1\")\n",
        "plt.plot(x, d2.pdf(x), color=\"blue\", label=\"pop 2\")\n",
        "plt.xlabel('Size')\n",
        "plt.ylabel('PDF')\n",
        "plt.grid()\n",
        "plt.legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23tURt3KTU0-"
      },
      "source": [
        "**Cálculo de la frontera óptima** (conocida también como _frontera bayesiana_)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsDaOt7otIrE"
      },
      "source": [
        "x = np.linspace(5,25,1000)\n",
        "minx = x[np.argmin(np.abs(d1.pdf(x)-d2.pdf(x)))]\n",
        "\n",
        "print (\"frontera óptima en %.2f\"%minx)\n",
        "\n",
        "x = np.linspace(0,30,100)\n",
        "plt.plot(x, d1.pdf(x), color=\"red\", label=\"pop 1\")\n",
        "plt.plot(x, d2.pdf(x), color=\"blue\", label=\"pop 2\")\n",
        "plt.axvline(minx, color=\"black\", label=u\"óptimo = %.2f\"%minx)\n",
        "plt.xlabel('Size')\n",
        "plt.ylabel('PDF')\n",
        "plt.grid()\n",
        "plt.legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDmE8KVMtXEB"
      },
      "source": [
        "Cálculo analítico de los errores de clasificación"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i56j_xf5tWaH"
      },
      "source": [
        "print (\"pop 1 error\", 1-d1.cdf(minx))\n",
        "print (\"pop 2 error\", d2.cdf(minx))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFrU-pa5SIkY"
      },
      "source": [
        "### Caso 1D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34WQy9SBtzCB"
      },
      "source": [
        "**Caso real**: Tenemos una muestra de los datos \n",
        "\n",
        "$\\rightarrow$ ¿Cómo determinamos donde poner la frontera?\n",
        "\n",
        "$\\rightarrow$ ¿Qué frontera qusiéramos obtener?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1g4z-Hmt6Zc"
      },
      "source": [
        "# Función para construir y graficar datasets con dos grupos, rojos y azules, en una dimensión\n",
        "def show_1D_dataset_samples(n, n_datasets=10, dot_alpha=.5, line_alpha=.5, figsize=(20,5)):\n",
        "    from sklearn.tree import DecisionTreeClassifier\n",
        "    plt.figure(figsize=figsize)\n",
        "    for i in range(n_datasets):\n",
        "\n",
        "        m1 = d1.rvs(n)\n",
        "        m2 = d2.rvs(n)\n",
        "        X = np.append(m1, m2).reshape(-1,1)\n",
        "        y = np.r_[[0]*len(m1)+[1]*len(m2)]\n",
        "        estimator = DecisionTreeClassifier(max_depth=1)\n",
        "        estimator.fit(X,y)\n",
        "        Xr = np.linspace(5, 30, 100).reshape(-1,1)\n",
        "        yr = estimator.predict(Xr)\n",
        "        plt.plot(Xr[yr==0], [i]*np.sum(yr==0), color=\"red\", alpha=line_alpha, lw=4)\n",
        "        plt.plot(Xr[yr==1], [i]*np.sum(yr==1), color=\"blue\", alpha=line_alpha, lw=4)\n",
        "        plt.scatter(m1, [i+.1]*len(m1), color=\"red\", alpha=dot_alpha, s=100)\n",
        "        plt.scatter(m2, [i+.1]*len(m2), color=\"blue\", alpha=dot_alpha, s=100)\n",
        "    plt.axis(\"off\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knqaRgu0t7ZJ"
      },
      "source": [
        "show_1D_dataset_samples(10, n_datasets=1, dot_alpha=.5, line_alpha=0, figsize=(20,1))\n",
        "plt.axis(\"on\")\n",
        "plt.ylim(.095, .105)\n",
        "plt.yticks([])\n",
        "plt.axhline(.1, color=\"black\", alpha=.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqjE_uV-0G71"
      },
      "source": [
        "Veamos como se puede mejorar la definición de la frontera a medida que aumentamos la cantidad de muestras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXaNZLRqt75l"
      },
      "source": [
        "show_1D_dataset_samples(10, dot_alpha=.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TK87NcF2t8Nu"
      },
      "source": [
        "show_1D_dataset_samples(100, dot_alpha=.05)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhWNrFP4t8hz"
      },
      "source": [
        "show_1D_dataset_samples(10000, dot_alpha=.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXrFYHtUPTPo"
      },
      "source": [
        "### Caso en 2D\n",
        "\n",
        "- En 2D, un modelo de clasificación **es una frontera** en el plano\n",
        "\n",
        "\n",
        "- **Objetivo:** hallar la frontera que produce menos error de clasificación\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7wNSKZYctIA"
      },
      "source": [
        "# copiando el archivo mlutils.py en la carpeta local\n",
        "!wget \"https://raw.githubusercontent.com/diplomado-bigdata-machinelearning-udea/Curso1/master/s06/mlutils.py\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QstlcMfPrYy"
      },
      "source": [
        "import mlutils \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwZUY6ggRUYw"
      },
      "source": [
        "Muestreo de datos \"pseudoaleatoria\" para clasificar en dos grupos: rojos, azules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgsqtEGUPrKL"
      },
      "source": [
        "mc = mlutils.Example_Bayes2DClassifier(mean0=[1.5, 2.5], cov0=[[0.1, 0.], [0., 0.1]],\n",
        "                                        mean1=[1.5, 2.], cov1=[[0.2,0.1],[0.1,.2]])\n",
        "X,y = mc.sample(100)\n",
        "mlutils.plot_2Ddata(X, y)\n",
        "plt.xlabel('Ancho')\n",
        "plt.ylabel('Alto')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYoGrKCHUES3"
      },
      "source": [
        "Clasificador lineal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wh2oDs3APqoX"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "estimador=LogisticRegression()\n",
        "estimador.fit(X,y)\n",
        "mlutils.plot_2Ddata_with_boundary(estimador.predict,X,y)\n",
        "plt.xlabel('Ancho')\n",
        "plt.ylabel('Alto')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swTUSq8LUITV"
      },
      "source": [
        "Clasificador bosque aleatorio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlabETUtQVGH"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "estimador=RandomForestClassifier()\n",
        "estimador.fit(X,y)\n",
        "mlutils.plot_2Ddata_with_boundary(estimador.predict,X,y)\n",
        "plt.xlabel('Ancho')\n",
        "plt.ylabel('Alto')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDkkUrgSULjV"
      },
      "source": [
        "Clasificador máquina de soporte vectorial"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7U8ayfuQXIP"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "estimador=SVC()\n",
        "estimador.fit(X,y)\n",
        "mlutils.plot_2Ddata_with_boundary(estimador.predict,X,y)\n",
        "plt.xlabel('Ancho')\n",
        "plt.ylabel('Alto')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8wPtmG1QdQ1"
      },
      "source": [
        "**Los algoritmos de machine learning:**\n",
        "\n",
        "- Los algoritmos de clasificación **calculan fronteras** entre los datos.\n",
        "- Parten de una muestra de los datos, **no de las distribuciones**.\n",
        "- Por tanto, **no conocemos** la forma de la frontera bayesiana. O sea, **partimos ciegos**!!!!!!!\n",
        "\n",
        "Para abordar esta situación, **cualquier algoritmo** tiene necesariamente que plantear una alguna suposición de base:\n",
        "\n",
        "- los datos vienen de distribuciones normales.\n",
        "- las columnas son estadísticamente independientes.\n",
        "- la frontera es lineal, o cuadrática.\n",
        "\n",
        "Teniendo esto en cuenta, y **partiendo de una muestra de los datos**:\n",
        "\n",
        "- el objetivo de un usuario de ML es acercarse lo más posible a la **frontera bayesiana** (que no sabemos cómo es).\n",
        "- distintos algoritmos ML tienen **distintas capacidades** para modelar fronteras (un clasificador lineal no puede captuarar una frontera cuadrática).\n",
        "- necesitamos dos tipos de herramientas:\n",
        "   - una buena colección de algoritmos ML.\n",
        "   - métodos para saber qué tan cerca estamos de la frontera bayesiana."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enrgilAHD5uI"
      },
      "source": [
        "<p><a name=\"terms\"></a></p>\n",
        "\n",
        "## 1.2. Terminología\n",
        "\n",
        "[Contenidos](#contents)\n",
        "\n",
        "Existen conceptos clave que sientan las bases para una mejor comprensión del ML. Aprenderemos la nomenclatura (términos estándar) que se utiliza para describir los datos, así como los términos utilizados para describir el aprendizaje y el modelado.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrS-hbyBKjbe"
      },
      "source": [
        "La estructura tradicional para los datos en el campo del ML tienen la siguiente forma:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qQuZF4_JVNw"
      },
      "source": [
        "![picture](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2013/12/Table-of-Data-Showing-an-Instance-Feature-and-Train-Test-Datasets.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9iBiOaPLwF3"
      },
      "source": [
        "* **Instancia**: A una sola fila de datos se le llama instancia. También se le conoce como una observación del dominio.\n",
        "* **Característica** (Feature): A una sola columna de datos se le llama característica. Es un componente de una observación y también se denomina atributo de una instancia de datos (La característica se suele asociar con el atributo y su valor, aunque la mayoría de las veces se usa atributo y característica indistintamente). Algunas características pueden ser entradas a un modelo (predictores) y otras pueden ser salidas o las características a predecir (también llamadas *labels*).\n",
        "* **Datos de entrenamiento**: Conjunto de datos que introducimos a nuestro algoritmo para entrenar nuestro modelo.\n",
        "* **Datos de prueba**: Conjunto de datos que utilizamos para validar la precisión de nuestro modelo pero que no se utiliza para entrenarlo. Puede llamarse conjunto de datos de validación.\n",
        "\n",
        "Otros términos que utilizaremos frecuentemente son:\n",
        "\n",
        "* **Parámetros del modelo**: Son aquellos que pertenecen al modelo utilizado para realizar el procedimiento de ajuste\n",
        "* **Hiperparámetros**: Es un parámetro de un algoritmo de aprendizaje (no del\n",
        "modelo). Como tal, no se ve afectado por el algoritmo de aprendizaje en sí; debe establecerse antes\n",
        "al entrenamiento y permanece constante durante el entrenamiento.\n",
        "* **Métrica**: Medida cuantitativa usada para evaluar el rendimiento del algoritmo.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jFVPnpF6alS"
      },
      "source": [
        "<p><a name=\"supML\"></a></p>\n",
        "\n",
        "## 1.3. Aprendizaje supervisado\n",
        "\n",
        "[Contenidos](#contents)\n",
        "\n",
        "El aprendizaje supervisado implica de alguna manera modelar la relación entre las características de los datos y alguna etiqueta asociada a estos; una vez que se determina el modelo, se puede usar para asociar etiquetas a datos nuevos y desconocidos. El aprendizaje supervisado se subdivide en clasificación y regresión: En clasificación se tienen etiquetas discretas, mientras que en la regresión, las etiquetas son cantidades continuas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuCUGAGcTPi5"
      },
      "source": [
        "### 1.3.1 Clasificación: Prediciendo etiquetas discretas.\n",
        "\n",
        "En clasificación, tenemos un conjunto de puntos etiquetados y deseamos utilizarlos para clasificar algunos puntos no etiquetados. Imaginemos que tenemos los datos que se muestran en la siguiente figura\n",
        "\n",
        "![picture](https://raw.githubusercontent.com/jakevdp/PythonDataScienceHandbook/master/notebooks/figures/05.01-classification-1.png)\n",
        "\n",
        "Tenemos dos características para cada punto, representadas por las posiciones (x, y). Además, tenemos una de las dos etiquetas de clases para cada punto, representadas por los colores de los puntos. A partir de estas características y etiquetas, nos gustaría crear un modelo que nos permita decidir si un nuevo punto debe etiquetarse como \"azul\" o \"rojo\". Un posible modelo es asumir que los dos grupos pueden separarse dibujando una línea recta a través del plano entre ellos, de modo que los puntos a cada lado de la línea caen en el mismo grupo.\n",
        "\n",
        "![picture](https://raw.githubusercontent.com/jakevdp/PythonDataScienceHandbook/master/notebooks/figures/05.01-classification-2.png)\n",
        "\n",
        "Este modelo es una versión cuantitativa del enunciado \"una línea recta separa las clases\", mientras que los parámetros del modelo son los números que describen la ubicación y orientación de la línea. Los valores óptimos para estos parámetros se aprenden de los datos (este es el \"aprendizaje\" en ML), que a menudo se denomina *entrenar el modelo*. Ahora que el modelo ha sido entrenado, se puede generalizar a datos nuevos sin etiquetas. Esta etapa generalmente se llama *predicción*.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiuuQ3avA715"
      },
      "source": [
        "<p><a name=\"nonsupML\"></a></p>\n",
        "\n",
        "## 1.4. Aprendizaje no supervisado\n",
        "\n",
        "[Contenidos](#contents)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywih9nDHp5kT"
      },
      "source": [
        "El aprendizaje no supervisado implica modelos que describen datos sin referencia a cualquier etiqueta conocida. Un caso común de aprendizaje no supervisado es el de *agrupamiento* (clustering) en el que los datos se asignan automáticamente a un cierto número de grupos discretos. Por ejemplo, podríamos tener algunos datos bidimensionales\n",
        "\n",
        "![picture](https://raw.githubusercontent.com/jakevdp/PythonDataScienceHandbook/master/notebooks/figures/05.01-clustering-1.png)\n",
        "\n",
        "A simple vista, está claro que cada uno de estos puntos es parte de un grupo distinto. Dada esta entrada, un modelo de agrupamiento utilizará la estructura intrínseca de los datos para determinar qué puntos están relacionados, con un resultado como el siguiente\n",
        "\n",
        "![picture](https://raw.githubusercontent.com/jakevdp/PythonDataScienceHandbook/master/notebooks/figures/05.01-clustering-2.png)\n",
        "\n",
        "Algunos algoritmos de aprendizaje no supervisado son:\n",
        "\n",
        "* Agrupamiento\n",
        "  * Jerárquico\n",
        "  * Basado en densidad\n",
        "  * k-medias\n",
        "* Detección de anomalías\n",
        "* Visualización y reducción dimensional\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edtIjb8esqNG"
      },
      "source": [
        "<p><a name=\"data\"></a></p>\n",
        "\n",
        "## 1.5. Observaciones acerca de los datos\n",
        "\n",
        "[Contenidos](#contents)\n",
        "\n",
        "* **Cantidad insuficiente de datos de entrenamiento**: Se necesitan muchos datos para que la mayoría de los algoritmos de ML funcionen correctamente. Incluso para problemas muy simples, generalmente se necesitan miles de ejemplos, y para problemas complejos como el reconocimiento de imágenes o de voz puede necesitarse millones de ejemplos. \n",
        "\n",
        "* **Datos de entrenamiento no representativos**: Para generalizar bien, es crucial que los datos de entrenamiento sean representativos de los nuevos casos que desea generalizar. Al usar un conjunto de entrenamiento no representativo, entrenamos un modelo con el cual es poco probable obtener predicciones precisas. Incluso muestras muy grandes pueden no ser representativas si el método de muestreo es defectuoso (sesgo en el muestreo).\n",
        "\n",
        "* **Datos de baja calidad**: Obviamente, si los datos de entrenamiento están llenos de errores, valores atípicos y ruido (por ejemplo, debido a mediciones de baja calidad), será más difícil para el algoritmo detectar los patrones subyacentes, por lo que es menos probable que funcione bien. ¡Siempre vale la pena dedicar tiempo a limpiar los datos de entrenamiento!\n",
        "\n",
        "* **Características irrelevantes**: El sistema solo será capaz de aprender si los datos de entrenamiento contienen suficientes características relevantes y no demasiadas irrelevantes. Una parte fundamental del éxito de un proyecto de ML es crear un buen conjunto de características para el entrenamiento. Este proceso se conoce como ingeniería de características. \n",
        "\n",
        "### ¡Los datos son más importantes que los algoritmos!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_A9WFzM546Y"
      },
      "source": [
        "<p><a name=\"sklearn\"></a></p>\n",
        "\n",
        "# 2. Introducción a Scikit-Learn\n",
        "\n",
        "[Contenidos](#contents) \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axPhkEXz5581"
      },
      "source": [
        "<p><a name=\"sklearnG\"></a></p>\n",
        "\n",
        "## 2.1 Generalidades\n",
        "\n",
        "Hay varias bibliotecas de Python que proporcionan implementaciones sólidas de una variedad de algoritmos de ML. Uno de los más conocidos es Scikit-Learn, un paquete que proporciona versiones eficientes de una gran cantidad de algoritmos comunes. Scikit-Learn se caracteriza por ser una API limpia, uniforme y optimizada, así como por una documentación en línea muy útil y completa. La API de Scikit-Learn está notablemente bien diseñada. Los principales principios de diseño son:\n",
        "\n",
        "* **Estimadores**: Cualquier objeto que pueda estimar algunos parámetros basados en un conjunto de datos se llama *estimador*. La estimación en sí misma se realiza mediante el método `fit()`, y solo toma un conjunto de datos como parámetro (o dos para algoritmos de aprendizaje supervisados; el segundo conjunto de datos contiene las etiquetas). Cualquier otro parámetro necesario para guiar el proceso de estimación se considera un hiperparámetro. Ya habíamos visto un ejemplo de un estimador cuando realizamos el llenado de valores nulos con `SimpleImputer` (La estrategía de llenado, en este caso \"most frequent\", es un ejemplo de un hiperparámetro)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGsZosQ14SfP"
      },
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "import pandas as pd\n",
        "\n",
        "imputer = SimpleImputer(strategy='most_frequent')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9rY3e6M4fUC"
      },
      "source": [
        "* **Transformadores**: Algunos estimadores (como el imputer) también pueden transformar un conjunto de datos; Estos se llaman transformadores. Una vez más, la API es bastante simple: la transformación se realiza mediante el método `transform()` con el conjunto de datos para transformar como parámetro. Los transformadores también tienen un método conveniente llamado `fit_transform()` que es equivalente a llamar a `fit()` y luego aplicar `transform()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V93uIM7u4iSx"
      },
      "source": [
        "imp = SimpleImputer(strategy='most_frequent')\n",
        "transformed_data = pd.DataFrame(imp.fit_transform(X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uphcWJo4j7x"
      },
      "source": [
        "* **Predictores**: Finalmente, algunos estimadores son capaces de hacer predicciones dado un conjunto de datos; Estos se conocen como predictores. Un predictor tiene un método `predict()` que toma un conjunto de datos de nuevas instancias y devuelve un conjunto de datos de predicciones correspondientes. También tiene un método `score()` que mide la calidad de las predicciones dado un conjunto de prueba. (El modelo `LinearRegression` que veremos más adelante es un ejemplo de un predictor)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThYEc56n6AP9"
      },
      "source": [
        "<p><a name=\"sklearnT\"></a></p>\n",
        "\n",
        "## 2.2 Tratamiento de datos\n",
        "\n",
        "ML se trata de crear modelos a partir de datos. Por esta razón, comenzaremos discutiendo algunos aspectos necesarios del tratamiento de los datos previos a la aplicación del algoritmo de ML en sklearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r66c6AJV8RyP"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df=pd.read_csv('https://raw.githubusercontent.com/diplomado-bigdata-machinelearning-udea/Curso1/master/s04/adult.csv') \n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZurcoswVSBB"
      },
      "source": [
        "<p><a name=\"sklearnTT\"></a></p>\n",
        "\n",
        "### 2.2.1 Datos de entrenamiento y de prueba\n",
        "\n",
        "Comencemos por crear la matrix de características **X** y el vector (serie) de etiquetas **y**. Supongamos que en este caso queremos predecir la característica \"age\":"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9oA7rngVgoR"
      },
      "source": [
        "X = df.drop(\"age\", axis=1)\n",
        "X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dwh3UeyYWIIa"
      },
      "source": [
        "y = df[\"age\"]\n",
        "y.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJOSK9ya_6aF"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "Generalmente se elige el 20% de los datos para la prueba y el resto para el entrenamiento. Los datos se deben dividir eligiendo los datos de manera aleatoria. Existen varias maneras de realizar este proceso\n",
        "\n",
        "   1) Crear una función que seleccione el 20% de los datos aleatoriamente.\n",
        "      \n",
        "   2) De la librería `sklearn.model_selection` usar las funciones:\n",
        "      \n",
        "      * train_test_split\n",
        "       \n",
        "      * StratifiedShuffleSplit \n",
        "      \n",
        "La primera función dividirá los datos en los mismos índices (esto es muy útil, por ejemplo, si tenemos un DataFrame separado para las etiquetas). La segunda función hace lo que se conoce como un muestreo estratificado, el cual se usa en el caso de que no haya suficientes datos y queramos evitar introducir sesgos\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVouMu_rT-lP"
      },
      "source": [
        "# Cuando tengamos una cantidad de datos tal que estemos seguros que no introduciremos un sesgo\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42) # random_state nos permite establecer una semilla \n",
        "\n",
        "print(x_train.shape,y_train.shape)\n",
        "print(x_test.shape,y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j26SK0N_ULDV"
      },
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "s = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=40)\n",
        "\n",
        "for train_index, test_index in s.split(X,y):\n",
        "  X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "  Y_train, Y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "  \n",
        "print(X_train.shape,Y_train.shape)\n",
        "print(X_test.shape,Y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2KQa9-FnJm4"
      },
      "source": [
        "<p><a name=\"sklearnCT\"></a></p>\n",
        "\n",
        "### 2.2.2 Conversión de variables categóricas\n",
        "\n",
        "Para la conversión de variables categóricas, sklearn ofrece los módulos `OrdinalEncoder` y `OneHotEncoder`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upAz1I2knn5C"
      },
      "source": [
        "df.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWwBMTVxnu8-"
      },
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "ordinal_encoder = OrdinalEncoder()\n",
        "df_cat_encoded = ordinal_encoder.fit_transform(df[[\"education\"]])\n",
        "df_cat_encoded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5zGziIYoe2T"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "cat_encoder = OneHotEncoder()\n",
        "#cat_encoder = OneHotEncoder(sparse=False)\n",
        "df_cat_1hot = cat_encoder.fit_transform(df[[\"sex\"]])\n",
        "df_cat_1hot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1aA6EiG2rIN"
      },
      "source": [
        "Por defecto el `OneHotEncoder` retorna una matriz dispersa (sparse). Muchos de los estimadores de Sklearn aceptan entradas de este tipo al ajustar y evaluar modelos. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keS9v4A2qiig"
      },
      "source": [
        "<p><a name=\"sklearnFS\"></a></p>\n",
        "\n",
        "### 2.2.3 Escalado de características"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYdSdabfqvxh"
      },
      "source": [
        "Los algoritmos de ML no funcionan bien cuando los atributos numéricos de entrada tienen escalas muy diferentes (o en algunos casos las entradas deben estar normalizadas). Para resolver esto, Scikit-Learn tiene dos funciones para que todos los atributos tengan la misma escala: \n",
        "\n",
        "* Normalización  \n",
        "$$ x'=\\frac{x-x_{min}}{x_{max}-x_{min}}.$$ \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCGH7VdFqvZP"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit_transform(df[[\"age\"]],df[[\"hoursperweek\"]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQv2DNcOvmlv"
      },
      "source": [
        "* Estandarización:\n",
        "$$ x'=\\frac{x-\\bar x}{\\sigma} $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORhvoYxNsrif"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "sscaler = StandardScaler()\n",
        "sscaler.fit_transform(df[[\"age\"]],df[[\"hoursperweek\"]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ma3-y7ZXvt7u"
      },
      "source": [
        "<p><a name=\"sklearnPL\"></a></p>\n",
        "\n",
        "### 3.2.4 Pipeline\n",
        "\n",
        "Un *Pipeline* es una secuencia de transformaciones que se utilizan para automatizar flujos de trabajo. Los pipelines son muy comunes en los sistemas de ML, ya que hay muchos datos para manipular y muchas transformaciones de datos para aplicar. Sklearn proporciona la clase `Pipeline`. En el siguiente ejemplo crearemos un pipeline para \n",
        "\n",
        "1) Reemplazar los valores nulos por la media \n",
        "\n",
        "2) Estandarizar los datos.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jULTvAWNxqBt"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "num_pipeline = Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
        "        ('std_scaler', StandardScaler()),\n",
        "    ])\n",
        "\n",
        "df_age_pipe = num_pipeline.fit_transform(df[[\"age\"]])\n",
        "\n",
        "trans_age = pd.DataFrame(df_age_pipe)\n",
        "print(trans_age.std(),trans_age.mean())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGTmN_HR97p6"
      },
      "source": [
        "<p><a name=\"taller\"></a></p>\n",
        "\n",
        "# 3. Taller\n",
        "\n",
        "[Contenidos](#contents) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qU41Rq0E8KoM"
      },
      "source": [
        "## Pregunta 1\n",
        "\n",
        "Cargar el dataset de autos, eliminar las filas con valores nulos y dejar solo las columnas con variables numéricas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCbQnD0JAqO-"
      },
      "source": [
        "Hacer doble click <b>aquí</b> para ver la solución:\n",
        "\n",
        "<!-- Respuesta:\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "archivo = 'https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DA0101EN/automobileEDA.csv'\n",
        "df = pd.read_csv(archivo)\n",
        "\n",
        "df.dropna(inplace=True)\n",
        "dfNum=df.select_dtypes(include=['float64','int64'])\n",
        "\n",
        "--->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZgt73LO8Sdl"
      },
      "source": [
        "## Pregunta 2\n",
        "\n",
        "Definir la variable a predecir $y$ como el precio y la matriz de características $X$ con el resto de variables numéricas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dt4CbOq5AxCg"
      },
      "source": [
        "Hacer doble click <b>aquí</b> para ver la solución:\n",
        "\n",
        "<!-- Respuesta:\n",
        "\n",
        "y=dfNum['price']\n",
        "X=dfNum.loc[:, dfNum.columns != 'price']\n",
        "#alternativa dfNum.drop('price',axis=1)\n",
        "\n",
        "\n",
        "--->"
      ]
    }
  ]
}